import streamlit as st
import lightgbm as lgb
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error


def modelisation():

    # Titre de la pr√©sentation
    st.title("üöÄ Pr√©sentation du Mod√®le LightGBM")

    # Introduction
    st.header("Introduction üåü")
    st.write(
        """
    Avant de s√©lectionner le mod√®le ci-dessous, nous avons utilis√© un LazyRegressor qui a g√©n√©r√© 29 mod√®les et leurs scores respectifs. Le LightGBM ayant obtenu les meilleurs r√©sultats, c'est celui que nous avons choisi pour ce projet.                   
    LightGBM est un framework de boosting de gradient d√©velopp√© par Microsoft. Il est con√ßu pour √™tre extr√™mement efficace, rapide et performant.
    """
    )

    # Fonctionnement de LightGBM
    st.header("Fonctionnement de LightGBM üõ†Ô∏è")

    st.write(
        """
    Le LightGBM Regressor fonctionne en combinant plusieurs techniques avanc√©es pour optimiser l'entra√Ænement et la pr√©cision des mod√®les de r√©gression :
    Gradient Boosting : Combine plusieurs mod√®les faibles s√©quentiellement pour cr√©er un mod√®le puissant.
             
    Exclusive Feature Bundling (EFB) : R√©duit la dimensionnalit√© en combinant des variables non chevauchantes.
             
    Gradient-based One-Side Sampling (GOSS) : Am√©liore l'efficacit√© de l'entra√Ænement en s√©lectionnant intelligemment les √©chantillons de donn√©es.
    Croissance verticale des arbres : Ajoute des niveaux de profondeur aux arbres de d√©cision pour am√©liorer les pr√©dictions.
    """
    )

    # Sch√©ma de fonctionnement de LightGBM
    chemin_image = os.path.join(
        os.getcwd(), "images", "A_stylized_diagram_illustrating_the_workflow_of_Li.jpg"
    )
    st.subheader("Sch√©ma de Fonctionnement üîç")
    st.image(
        chemin_image,
        caption="Sch√©ma de Fonctionnement de LightGBM",
        use_column_width=True,
    )

    # Avantages de LightGBM
    st.header("Avantages de LightGBM üí°")

    st.write(
        """
    - **Vitesse et Efficacit√©** : LightGBM est con√ßu pour √™tre tr√®s rapide et efficace.
    - **Pr√©cision** : Gr√¢ce √† ses techniques avanc√©es de boosting, il offre une grande pr√©cision.
    - **Scalabilit√©** : Il peut g√©rer des ensembles de donn√©es volumineux avec de nombreuses fonctionnalit√©s.
    - **Support des Valeurs Manquantes** : LightGBM g√®re nativement les valeurs manquantes.
    """
    )

    # Sch√©ma des avantages
    st.subheader("Avantages en un coup d'≈ìil üìä")
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.barh(
        ["Vitesse", "Pr√©cision", "Scalabilit√©", "Support des Valeurs Manquantes"],
        [90, 85, 95, 80],
        color="skyblue",
    )
    ax.set_xlabel("Importance (%)")
    ax.set_title("Avantages de LightGBM")
    st.pyplot(fig)
    plt.close(fig)  # Ferme la figure apr√®s l'affichage

    # Interactivit√© avec l'utilisateur
    st.header("Essayez par vous-m√™me ! üéÆ")

    # Slider interactif pour ajuster les param√®tres (exemple)
    max_depth = st.slider("Choisissez la profondeur maximale de l'arbre", 1, 20, 6)
    learning_rate = st.slider("Choisissez le taux d'apprentissage", 0.01, 0.5, 0.1)

    # Chargement des donn√©es
    data = fetch_california_housing()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = pd.Series(data.target, name="target")

    # Division des donn√©es
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Cr√©ation et entra√Ænement du mod√®le avec les param√®tres ajust√©s
    model = lgb.LGBMRegressor(max_depth=max_depth, learning_rate=learning_rate)
    model.fit(X_train, y_train)

    # Pr√©diction et √©valuation
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)

    # Affichage des r√©sultats
    st.write(
        f"Erreur Quadratique Moyenne (MSE) avec profondeur {max_depth} et taux d'apprentissage {learning_rate}: {mse:.2f}"
    )

    # Comparaison des valeurs pr√©dites et r√©elles
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.scatter(y_test, y_pred, alpha=0.3)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", lw=2)
    ax.set_xlabel("Valeurs r√©elles")
    ax.set_ylabel("Valeurs pr√©dites")
    ax.set_title("Comparaison des valeurs r√©elles et pr√©dites")
    st.pyplot(fig)
    plt.close(fig)  # Ferme la figure apr√®s l'affichage

    # Conclusion
    st.header("Conclusion üéØ")
    st.write(
        """
            LightGBM est un outil puissant pour les t√¢ches de r√©gression et de classification, particuli√®rement adapt√© aux grands ensembles de donn√©es.

            Score du mod√®le avant feature engineering :

            LGBMRegressor R¬≤ : Moyenne = 0.3107, √âcart-type = 0.0151
            LGBMRegressor MSE : Moyenne = 0.0400, √âcart-type = 0.0027
            LGBMRegressor MAE : Moyenne = 0.1432, √âcart-type = 0.0043
            
            Score du mod√®le apr√®s feature engineering :

            LGBMRegressor R¬≤ : Moyenne = 0.9880, √âcart-type = 0.0035
            LGBMRegressor MSE : Moyenne = 0.0007, √âcart-type = 0.0002
            LGBMRegressor MAE : Moyenne = 0.0132, √âcart-type = 0.0013
"""
    )

    # Ajout d'un GIF fun li√© aux jeux vid√©o
    st.markdown(
        """
    <div style="display: flex; justify-content: center;">
        <iframe src="https://giphy.com/embed/mHv5sLKI1b1I8r4wmp" width="680" height="370" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
    </div>
    """,
        unsafe_allow_html=True,
    )
